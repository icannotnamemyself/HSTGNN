{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import signal\n",
    "import threading\n",
    "import time\n",
    "import hashlib\n",
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "####\n",
    "from typing import Dict, List, Type, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import MeanSquaredError, MetricCollection, MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch_timeseries.data.scaler import *\n",
    "from torch_timeseries.datasets import *\n",
    "from torch_timeseries.experiments.experiment import Experiment\n",
    "\n",
    "from torch_timeseries.datasets.dataset import TimeSeriesDataset\n",
    "from torch_timeseries.datasets.splitter import SequenceRandomSplitter, SequenceSplitter\n",
    "from torch_timeseries.datasets.dataloader import (\n",
    "    ChunkSequenceTimefeatureDataLoader,\n",
    "    DDPChunkSequenceTimefeatureDataLoader,\n",
    ")\n",
    "from torch_timeseries.datasets.wrapper import MultiStepTimeFeatureSet\n",
    "from torch_timeseries.models.Informer import Informer\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, Subset\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn as nn\n",
    "from dataclasses import asdict,dataclass\n",
    "\n",
    "from torch_timeseries.nn.metric import R2, Corr, TrendAcc,RMSE, compute_corr, compute_r2\n",
    "from torch_timeseries.metrics.masked_mape import MaskedMAPE\n",
    "from torch_timeseries.utils.early_stopping import EarlyStopping\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from class_resolver.contrib.torch import activation_resolver\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn import FAConv,HeteroConv\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "\n",
    "\n",
    "from torch_timeseries.layers.tcn_output8 import TCNOuputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EGraphSage(MessagePassing):\n",
    "    \"\"\"Non-minibatch version of GraphSage.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 edge_channels=1, activation='elu', edge_mode=1,\n",
    "                 normalize_emb=False, aggr='add'):\n",
    "        super(EGraphSage, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_channels = edge_channels\n",
    "        self.edge_mode = edge_mode\n",
    "        self.act = activation_resolver.make(activation)\n",
    "\n",
    "        if edge_mode == 0:\n",
    "            self.message_lin = nn.Linear(in_channels, out_channels)\n",
    "            self.attention_lin = nn.Linear(2*in_channels+edge_channels, 1)\n",
    "        elif edge_mode == 1:\n",
    "            self.message_lin = nn.Linear(in_channels+edge_channels, out_channels)\n",
    "        elif edge_mode == 2:\n",
    "            self.message_lin = nn.Linear(2*in_channels+edge_channels, out_channels)\n",
    "        elif edge_mode == 3:\n",
    "            self.message_lin = nn.Sequential(\n",
    "                    nn.Linear(2*in_channels+edge_channels, out_channels),\n",
    "                    self.act,\n",
    "                    nn.Linear(out_channels, out_channels),\n",
    "                    )\n",
    "        elif edge_mode == 4:\n",
    "            self.message_lin = nn.Linear(in_channels, out_channels*edge_channels)\n",
    "        elif edge_mode == 5:\n",
    "            self.message_lin = nn.Linear(2*in_channels, out_channels*edge_channels)\n",
    "\n",
    "        self.agg_lin = nn.Linear(in_channels+out_channels, out_channels)\n",
    "\n",
    "        self.message_activation = self.act\n",
    "        self.update_activation = self.act\n",
    "        self.normalize_emb = normalize_emb\n",
    "\n",
    "    def forward(self, x, edge_attr, edge_index):\n",
    "        num_nodes = x.size(0)\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, size=(num_nodes, num_nodes))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr, edge_index, size):\n",
    "        # x_j has shape [E, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        if self.edge_mode == 0:\n",
    "            attention = self.attention_lin(torch.cat((x_i,x_j, edge_attr),dim=-1))\n",
    "            m_j = attention * self.message_activation(self.message_lin(x_j))\n",
    "        elif self.edge_mode == 1:\n",
    "            m_j = torch.cat((x_j, edge_attr),dim=-1)\n",
    "            m_j = self.message_activation(self.message_lin(m_j))\n",
    "        elif self.edge_mode == 2 or self.edge_mode == 3:\n",
    "            m_j = torch.cat((x_i,x_j, edge_attr),dim=-1)\n",
    "            m_j = self.message_activation(self.message_lin(m_j))\n",
    "        elif self.edge_mode == 4:\n",
    "            E = x_j.shape[0]\n",
    "            w = self.message_lin(x_j)\n",
    "            w = self.message_activation(w)\n",
    "            w = torch.reshape(w, (E,self.out_channels,self.edge_channels))\n",
    "            m_j = torch.bmm(w, edge_attr.unsqueeze(-1)).squeeze(-1)\n",
    "        elif self.edge_mode == 5:\n",
    "            E = x_j.shape[0]\n",
    "            w = self.message_lin(torch.cat((x_i,x_j),dim=-1))\n",
    "            w = self.message_activation(w)\n",
    "            w = torch.reshape(w, (E,self.out_channels,self.edge_channels))\n",
    "            m_j = torch.bmm(w, edge_attr.unsqueeze(-1)).squeeze(-1)\n",
    "        return m_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # x has shape [N, in_channels]\n",
    "        aggr_out = self.update_activation(self.agg_lin(torch.cat((aggr_out, x),dim=-1)))\n",
    "        if self.normalize_emb:\n",
    "            aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGraphSage(MessagePassing):\n",
    "    \"\"\"Non-minibatch version of GraphSage.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 activation,\n",
    "                 normalize_emb,\n",
    "                 aggr):\n",
    "        super(PGraphSage, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.message_lin = nn.Linear(in_channels, out_channels)\n",
    "        self.agg_lin = nn.Linear(in_channels+out_channels, out_channels)\n",
    "\n",
    "        self.update_activation = get_activation(activation)\n",
    "        self.normalize_emb = normalize_emb\n",
    "\n",
    "    def forward(self, x, edge_attr, edge_index):\n",
    "        num_nodes = x.size(0)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, size=(num_nodes, num_nodes))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr, edge_index, size):\n",
    "        # x_j has shape [E, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        m_j = edge_attr.view(-1,1)*x_j\n",
    "        m_j = self.message_lin(m_j)\n",
    "        return m_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # x has shape [N, in_channels]\n",
    "        aggr_out = self.update_activation(self.agg_lin(torch.cat((aggr_out, x),dim=-1)))\n",
    "        if self.normalize_emb:\n",
    "            aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "\n",
    "def get_activation(activation):\n",
    "    if activation == 'relu':\n",
    "        return torch.nn.ReLU()\n",
    "    elif activation == 'prelu':\n",
    "        return torch.nn.PReLU()\n",
    "    elif activation == 'tanh':\n",
    "        return torch.nn.Tanh()\n",
    "    elif (activation is None) or (activation == 'none'):\n",
    "        return torch.nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                node_input_dim, edge_input_dim,model_types,\n",
    "                node_dim=64, edge_dim=64, edge_mode=1,\n",
    "                dropout=0.0, activation='relu',\n",
    "                concat_states=True, node_post_mlp_hiddens=[64],\n",
    "                 aggr='sum', argu_type=\"friend_network\"\n",
    "                ):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.concat_states = concat_states\n",
    "        self.model_types = model_types\n",
    "        self.argu_type = argu_type\n",
    "        self.gnn_layer_num = len(model_types)\n",
    "        self.normalize_embs = [True] * len(model_types)\n",
    "\n",
    "        # convs\n",
    "        self.convs = self.build_convs(node_input_dim, edge_input_dim,\n",
    "                                    node_dim, edge_dim, edge_mode,\n",
    "                                    model_types, self.normalize_embs, activation, aggr)\n",
    "        # post node update\n",
    "        if concat_states:\n",
    "            self.node_post_mlp = self.build_node_post_mlp(int(node_dim*len(model_types)), int(node_dim*len(model_types)), node_post_mlp_hiddens, dropout, activation)\n",
    "        else:  #default\n",
    "            self.node_post_mlp = self.build_node_post_mlp(node_dim, node_dim, node_post_mlp_hiddens, dropout, activation)\n",
    "\n",
    "        self.edge_update_mlps = self.build_edge_update_mlps(node_dim, edge_input_dim, edge_dim, self.gnn_layer_num, activation)\n",
    "        if self.argu_type == 'friend_network':\n",
    "            self.sage = self.build_friend_network_convs(node_dim,node_dim,activation,self.normalize_embs,aggr)\n",
    "        # elif self.argu_type == 'heterogeneous_network':\n",
    "        #     self.weight_han = self.build_heterogeneous_network_convs(node_dim,node_dim,activation,normalize_embs,aggr)\n",
    "\n",
    "\n",
    "    def build_friend_network_convs(self, input_dim, output_dim, activation, normalize_embs, aggr):\n",
    "        convs = nn.ModuleList()\n",
    "        conv = PGraphSage(input_dim,output_dim,activation,normalize_embs,aggr)\n",
    "        convs.append(conv)\n",
    "        return convs\n",
    "\n",
    "    # def build_heterogeneous_network_convs(self, input_dim, output_dim, activation, normalize_embs, aggr):\n",
    "    #     conv = WeightedHAN(input_dim,input_dim,output_dim, n_layers=1)\n",
    "    #     return conv\n",
    " \n",
    "    def build_node_post_mlp(self, input_dim, output_dim, hidden_dims, dropout, activation):\n",
    "        if 0 in hidden_dims:\n",
    "            return get_activation('none')\n",
    "        else:\n",
    "            layers = []\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layer = nn.Sequential(\n",
    "                            nn.Linear(input_dim, hidden_dim),\n",
    "                            get_activation(activation),\n",
    "                            nn.Dropout(dropout),\n",
    "                            )\n",
    "                layers.append(layer)\n",
    "                input_dim = hidden_dim\n",
    "            layer = nn.Linear(input_dim, output_dim)\n",
    "            layers.append(layer)\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "    def build_convs(self, node_input_dim, edge_input_dim,\n",
    "                     node_dim, edge_dim, edge_mode,\n",
    "                     model_types, normalize_embs, activation, aggr):\n",
    "        convs = nn.ModuleList()\n",
    "        conv = self.build_conv_model(model_types[0],node_input_dim,node_dim,\n",
    "                                    edge_input_dim, edge_mode, normalize_embs[0], activation, aggr)\n",
    "        convs.append(conv)\n",
    "        for l in range(1,len(model_types)):\n",
    "            conv = self.build_conv_model(model_types[l],node_dim, node_dim,\n",
    "                                    edge_dim, edge_mode, normalize_embs[l], activation, aggr)\n",
    "            convs.append(conv)\n",
    "        return convs\n",
    "\n",
    "    def build_conv_model(self, model_type, node_in_dim, node_out_dim, edge_dim, edge_mode, normalize_emb, activation, aggr):\n",
    "        if model_type == 'GCN':\n",
    "            return pyg_nn.GCNConv(node_in_dim,node_out_dim)\n",
    "        elif model_type == 'GraphSage':\n",
    "            return pyg_nn.SAGEConv(node_in_dim,node_out_dim)\n",
    "        elif model_type == 'GAT':\n",
    "            return pyg_nn.GATConv(node_in_dim,node_out_dim)\n",
    "        # elif model_type == 'EGCN':\n",
    "        #     return EGCNConv(node_in_dim,node_out_dim,edge_dim,edge_mode)\n",
    "        elif model_type == 'EGSAGE':\n",
    "            return EGraphSage(node_in_dim,node_out_dim,edge_dim,activation,edge_mode,normalize_emb, aggr)\n",
    "        # elif model_type == 'WeightedHAN':\n",
    "        #     return EWeightedHAN(node_in_dim,node_in_dim,node_out_dim,edge_dim,n_layers=1)\n",
    "\n",
    "    def build_edge_update_mlps(self, node_dim, edge_input_dim, edge_dim, gnn_layer_num, activation):\n",
    "        edge_update_mlps = nn.ModuleList()\n",
    "        edge_update_mlp = nn.Sequential(\n",
    "                nn.Linear(node_dim+node_dim+edge_input_dim,edge_dim),\n",
    "                get_activation(activation),\n",
    "                )\n",
    "        edge_update_mlps.append(edge_update_mlp)\n",
    "        for l in range(1,gnn_layer_num):\n",
    "            edge_update_mlp = nn.Sequential(\n",
    "                nn.Linear(node_dim+node_dim+edge_dim,edge_dim),\n",
    "                get_activation(activation),\n",
    "                )\n",
    "            edge_update_mlps.append(edge_update_mlp)\n",
    "        return edge_update_mlps\n",
    "    \n",
    "\n",
    "    def update_edge_attr(self, x, edge_attr, edge_index, mlp):\n",
    "        x_i = x[edge_index[0],:]\n",
    "        x_j = x[edge_index[1],:]\n",
    "        edge_attr = mlp(torch.cat((x_i,x_j,edge_attr),dim=-1))\n",
    "        return edge_attr\n",
    "\n",
    "    def forward(self, x, edge_attr, edge_index):\n",
    "        # x : (N+M x M) node emebedding\n",
    "        if self.concat_states:\n",
    "            concat_x = []\n",
    "        for l,(conv_name,conv) in enumerate(zip(self.model_types,self.convs)):\n",
    "            if conv_name == 'EGCN' or conv_name == 'EGSAGE'  :\n",
    "                x = conv(x, edge_attr, edge_index)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            if self.concat_states:\n",
    "                concat_x.append(x)\n",
    "            edge_attr = self.update_edge_attr(x, edge_attr, edge_index, self.edge_update_mlps[l])\n",
    "        if self.concat_states:\n",
    "            x = torch.cat(concat_x, 1)\n",
    "        return x\n",
    "\n",
    "    def F_augmentation(self, x, edge_attr, edge_index, num_obs):\n",
    "        if self.argu_type == 'friend_network':\n",
    "            x_sage = x[:num_obs]\n",
    "            for l, conv in enumerate(self.sage):\n",
    "                x_sage = conv(x_sage, edge_attr, edge_index)\n",
    "            x[:num_obs] = x_sage\n",
    "            return x\n",
    "        # elif self.argu_type == 'heterogeneous_network':\n",
    "        #     x = self.weight_han(x, edge_attr, edge_index,num_obs)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "         \t\tinput_dims, output_dim,\n",
    "         \t\thidden_layer_sizes=(64,),\n",
    "         \t\thidden_activation='relu',\n",
    "         \t\toutput_activation=None,\n",
    "                dropout=0.):\n",
    "        super(MLPNet, self).__init__()\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        input_dim = np.sum(input_dims)\n",
    "\n",
    "        for layer_size in hidden_layer_sizes:\n",
    "            hidden_dim = layer_size\n",
    "            layer = nn.Sequential(\n",
    "                        nn.Linear(input_dim, hidden_dim),\n",
    "                        get_activation(hidden_activation),\n",
    "                        nn.Dropout(dropout),\n",
    "                        )\n",
    "            layers.append(layer)\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        layer = nn.Sequential(\n",
    "                        nn.Linear(input_dim, output_dim),\n",
    "                        get_activation(output_activation),\n",
    "                        )\n",
    "        layers.append(layer)\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if torch.is_tensor(inputs):\n",
    "            inputs = [inputs]\n",
    "        input_var = torch.cat(inputs,-1)\n",
    "        for layer in self.layers:\n",
    "            input_var = layer(input_var)\n",
    "        return input_var\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
       "         [5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8]]),\n",
       " tensor([0.7279, 0.6376, 0.8657, 0.9437, 0.1777, 0.7556, 0.4771, 0.8118, 0.2633,\n",
       "         0.8074, 0.4561, 0.5428, 0.7921, 0.2462, 0.1073, 0.7869, 0.2728, 0.9150,\n",
       "         0.7889, 0.6184]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "# 假设的 d[i]，你需要替换成你自己的数据\n",
    "# 这里使用随机数据来模拟\n",
    "O, N = 5, 4  # 假设的 O 和 N 的大小\n",
    "d_i = torch.rand(O, N)  # 随机生成 O x N 的矩阵\n",
    "\n",
    "# 提取边的索引和边的属性\n",
    "edge_index, edge_attr = dense_to_sparse(d_i)\n",
    "edge_index[1] = edge_index[1] + O\n",
    "\n",
    "# 打印结果查看\n",
    "edge_index, edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fc7f7aafd30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "class GNNPredictor(torch.nn.Module):\n",
    "    def __init__(self, feature_number, output_length, device, layers=3):\n",
    "        super(GNNPredictor, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.model =  GNNStack(feature_number, 1, model_types=[\"EGSAGE\"]*layers)\n",
    "        \n",
    "        self.hot_encoding_matrix = torch.eye(feature_number).detach()\n",
    "        self.temporal_encoding_matrix = torch.arange(0, output_length) / output_length # \n",
    "        self.temporal_encoding_matrix = self.temporal_encoding_matrix.unsqueeze(1).repeat(1, feature_number)\n",
    "        # row_indices = torch.arange(feature_number).repeat_interleave(output_length)\n",
    "        # col_indices = torch.arange(feature_number, feature_number + output_length).repeat(feature_number) \n",
    "        # edge_index = torch.stack([row_indices, col_indices], dim=0).detach()\n",
    "        # # bi_edge_index = torch.cat([edge_index, torch.flip(edge_index, [0])], dim=1)\n",
    "        # self.sage_edge_index= torch.cat([edge_index, torch.flip(edge_index, [0])], dim=1).detach()\n",
    "        # 复制 edge_attr 以匹配双向边 .to(device)\n",
    "        \n",
    "        self.x_input = torch.concat([self.hot_encoding_matrix, self.temporal_encoding_matrix], dim=0).to(device).detach()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.mlp = MLPNet(64* layers * 2, 1)\n",
    "    def forward(self, d):\n",
    "        # d : p - t difference\n",
    "        # d: (B, O, N)\n",
    "        \n",
    "        B, O, N= d.size()\n",
    "        x_embed = self.x_input.detach()\n",
    "        \n",
    "        pred_ds = []\n",
    "        \n",
    "        for bi in range(B):\n",
    "            # 提取边的索引和边的属性\n",
    "            edge_index, edge_attr = dense_to_sparse(d[bi])\n",
    "            edge_index[1] = edge_index[1] + O\n",
    "            edge_attr = edge_attr.unsqueeze(1)\n",
    "            \n",
    "\n",
    "            x_impute = self.model(x_embed, edge_attr, edge_index)\n",
    "\n",
    "            pred = self.mlp([x_impute[edge_index[0]], x_impute[edge_index[1]]])\n",
    "            pred = pred.reshape(O, N)\n",
    "            pred_ds.append(pred)\n",
    "       \n",
    "        return  torch.stack(pred_ds)    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "from torch_timeseries.models import NLinear\n",
    "\n",
    "class NlinearGNNEnhancer(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, enc_in, device, gnn_layers=3, individual=False, normalization=True):\n",
    "        super(NlinearGNNEnhancer, self).__init__()\n",
    "\n",
    "        self.nlinear = NLinear(seq_len, pred_len, enc_in, individual, normalization)\n",
    "        self.gnn = GNNPredictor(enc_in, pred_len,device, layers=gnn_layers)\n",
    "    \n",
    "    def train_predict(self, x, y):\n",
    "        p = self.nlinear(x)\n",
    "        d = self.gnn(p.detach())\n",
    "        # d = self.gnn(p)\n",
    "        \n",
    "        return p, d \n",
    "    \n",
    "    \n",
    "    def inference_predict(self, x):\n",
    "        p = self.nlinear(x)\n",
    "        d = self.gnn(p)\n",
    "        \n",
    "        return p + d\n",
    "    \n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class NlinearGNNEnhanceExperiment(Experiment):\n",
    "    \n",
    "    \n",
    "    gnn_layers : int = 3\n",
    "\n",
    "    def _process_one_batch(self, batch_x, batch_y, batch_x_date_enc, batch_y_date_enc):\n",
    "        # inputs:\n",
    "        # batch_x: (B, T, N)\n",
    "        # batch_y: (B, O, N)\n",
    "        # ouputs:\n",
    "        # - pred: (B, N)/(B, O, N)\n",
    "        # - label: (B, N)/(B, O, N)\n",
    "        batch_size = batch_x.size(0)\n",
    "        batch_x = batch_x.to(self.device, dtype=torch.float32)\n",
    "        batch_y = batch_y.to(self.device, dtype=torch.float32)\n",
    "        batch_x_date_enc = batch_x_date_enc.to(self.device).float()\n",
    "        batch_y_date_enc = batch_y_date_enc.to(self.device).float()\n",
    "        batch_x = batch_x\n",
    "        outputs = self.model.inference_predict(batch_x)  # torch.Size([batch_size, num_nodes])\n",
    "        # single step prediction\n",
    "        return outputs, batch_y\n",
    "\n",
    "    def _process_one_batch_train(self, batch_x, batch_y, batch_x_date_enc, batch_y_date_enc):\n",
    "        # inputs:\n",
    "        # batch_x: (B, T, N)\n",
    "        # batch_y: (B, O, N)\n",
    "        # ouputs:\n",
    "        # - pred: (B, N)/(B, O, N)\n",
    "        # - label: (B, N)/(B, O, N)\n",
    "        batch_size = batch_x.size(0)\n",
    "        batch_x = batch_x.to(self.device, dtype=torch.float32)\n",
    "        batch_y = batch_y.to(self.device, dtype=torch.float32)\n",
    "        batch_x_date_enc = batch_x_date_enc.to(self.device).float()\n",
    "        batch_y_date_enc = batch_y_date_enc.to(self.device).float()\n",
    "        batch_x = batch_x\n",
    "        p, d = self.model.train_predict(batch_x, batch_y)  # torch.Size([batch_size, num_nodes])\n",
    "        # single step prediction\n",
    "        return p, d , batch_y\n",
    "\n",
    "\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.model = NlinearGNNEnhancer(\n",
    "            seq_len=self.windows,\n",
    "            pred_len=self.pred_len,\n",
    "            enc_in=self.dataset.num_features,\n",
    "            gnn_layers=self.gnn_layers,\n",
    "            individual=True,\n",
    "            normalization=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.d_loss = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "    def _train(self):\n",
    "        with torch.enable_grad(), tqdm(total=self.train_steps) as progress_bar:\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for i, (\n",
    "                batch_x,\n",
    "                batch_y,\n",
    "                origin_y,\n",
    "                batch_x_date_enc,\n",
    "                batch_y_date_enc,\n",
    "            ) in enumerate(self.train_loader):\n",
    "                origin_y = origin_y.to(self.device)\n",
    "                self.model_optim.zero_grad()\n",
    "                pred, pred_d, true = self._process_one_batch_train(\n",
    "                    batch_x, batch_y, batch_x_date_enc, batch_y_date_enc\n",
    "                )\n",
    "                if self.invtrans_loss:\n",
    "                    pred = self.scaler.inverse_transform(pred)\n",
    "                    true = origin_y\n",
    "                    \n",
    "                loss1 = self.d_loss(pred_d, pred.detach() - true)\n",
    "                loss2 = self.loss_func(pred, true)\n",
    "\n",
    "                loss2.backward()\n",
    "                loss1.backward(retain_graph=True)\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), self.max_grad_norm\n",
    "                )\n",
    "                progress_bar.update(batch_x.size(0))\n",
    "                train_loss.append(loss1.item() + loss2.item())\n",
    "                progress_bar.set_postfix(\n",
    "                    lr=self.model_optim.param_groups[0][\"lr\"],\n",
    "                    epoch=self.current_epoch,\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "                self.model_optim.step()\n",
    "            return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train steps: 1325\n",
      "val steps: 333\n",
      "test steps: 137\n",
      "torch.get_default_dtype() torch.float32\n",
      "Creating running results saving dir: './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "run : 0 in seed: 456\n",
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|        nlinear.Linear.0.weight        |     40     |\n",
      "|         nlinear.Linear.0.bias         |     1      |\n",
      "|        nlinear.Linear.1.weight        |     40     |\n",
      "|         nlinear.Linear.1.bias         |     1      |\n",
      "|        nlinear.Linear.2.weight        |     40     |\n",
      "|         nlinear.Linear.2.bias         |     1      |\n",
      "|        nlinear.Linear.3.weight        |     40     |\n",
      "|         nlinear.Linear.3.bias         |     1      |\n",
      "|        nlinear.Linear.4.weight        |     40     |\n",
      "|         nlinear.Linear.4.bias         |     1      |\n",
      "|        nlinear.Linear.5.weight        |     40     |\n",
      "|         nlinear.Linear.5.bias         |     1      |\n",
      "|        nlinear.Linear.6.weight        |     40     |\n",
      "|         nlinear.Linear.6.bias         |     1      |\n",
      "|        nlinear.Linear.7.weight        |     40     |\n",
      "|         nlinear.Linear.7.bias         |     1      |\n",
      "|        nlinear.Linear.8.weight        |     40     |\n",
      "|         nlinear.Linear.8.bias         |     1      |\n",
      "|        nlinear.Linear.9.weight        |     40     |\n",
      "|         nlinear.Linear.9.bias         |     1      |\n",
      "|        nlinear.Linear.10.weight       |     40     |\n",
      "|         nlinear.Linear.10.bias        |     1      |\n",
      "|        nlinear.Linear.11.weight       |     40     |\n",
      "|         nlinear.Linear.11.bias        |     1      |\n",
      "|        nlinear.Linear.12.weight       |     40     |\n",
      "|         nlinear.Linear.12.bias        |     1      |\n",
      "|        nlinear.Linear.13.weight       |     40     |\n",
      "|         nlinear.Linear.13.bias        |     1      |\n",
      "|        nlinear.Linear.14.weight       |     40     |\n",
      "|         nlinear.Linear.14.bias        |     1      |\n",
      "|        nlinear.Linear.15.weight       |     40     |\n",
      "|         nlinear.Linear.15.bias        |     1      |\n",
      "|        nlinear.Linear.16.weight       |     40     |\n",
      "|         nlinear.Linear.16.bias        |     1      |\n",
      "|        nlinear.Linear.17.weight       |     40     |\n",
      "|         nlinear.Linear.17.bias        |     1      |\n",
      "|        nlinear.Linear.18.weight       |     40     |\n",
      "|         nlinear.Linear.18.bias        |     1      |\n",
      "|        nlinear.Linear.19.weight       |     40     |\n",
      "|         nlinear.Linear.19.bias        |     1      |\n",
      "|        nlinear.Linear.20.weight       |     40     |\n",
      "|         nlinear.Linear.20.bias        |     1      |\n",
      "|        nlinear.Linear.21.weight       |     40     |\n",
      "|         nlinear.Linear.21.bias        |     1      |\n",
      "|        nlinear.Linear.22.weight       |     40     |\n",
      "|         nlinear.Linear.22.bias        |     1      |\n",
      "|        nlinear.Linear.23.weight       |     40     |\n",
      "|         nlinear.Linear.23.bias        |     1      |\n",
      "|        nlinear.Linear.24.weight       |     40     |\n",
      "|         nlinear.Linear.24.bias        |     1      |\n",
      "|        nlinear.Linear.25.weight       |     40     |\n",
      "|         nlinear.Linear.25.bias        |     1      |\n",
      "|        nlinear.Linear.26.weight       |     40     |\n",
      "|         nlinear.Linear.26.bias        |     1      |\n",
      "|        nlinear.Linear.27.weight       |     40     |\n",
      "|         nlinear.Linear.27.bias        |     1      |\n",
      "|        nlinear.Linear.28.weight       |     40     |\n",
      "|         nlinear.Linear.28.bias        |     1      |\n",
      "|        nlinear.Linear.29.weight       |     40     |\n",
      "|         nlinear.Linear.29.bias        |     1      |\n",
      "|        nlinear.Linear.30.weight       |     40     |\n",
      "|         nlinear.Linear.30.bias        |     1      |\n",
      "|        nlinear.Linear.31.weight       |     40     |\n",
      "|         nlinear.Linear.31.bias        |     1      |\n",
      "|        nlinear.Linear.32.weight       |     40     |\n",
      "|         nlinear.Linear.32.bias        |     1      |\n",
      "|        nlinear.Linear.33.weight       |     40     |\n",
      "|         nlinear.Linear.33.bias        |     1      |\n",
      "|        nlinear.Linear.34.weight       |     40     |\n",
      "|         nlinear.Linear.34.bias        |     1      |\n",
      "|        nlinear.Linear.35.weight       |     40     |\n",
      "|         nlinear.Linear.35.bias        |     1      |\n",
      "|        nlinear.Linear.36.weight       |     40     |\n",
      "|         nlinear.Linear.36.bias        |     1      |\n",
      "|        nlinear.Linear.37.weight       |     40     |\n",
      "|         nlinear.Linear.37.bias        |     1      |\n",
      "|        nlinear.Linear.38.weight       |     40     |\n",
      "|         nlinear.Linear.38.bias        |     1      |\n",
      "|        nlinear.Linear.39.weight       |     40     |\n",
      "|         nlinear.Linear.39.bias        |     1      |\n",
      "|        nlinear.Linear.40.weight       |     40     |\n",
      "|         nlinear.Linear.40.bias        |     1      |\n",
      "|        nlinear.Linear.41.weight       |     40     |\n",
      "|         nlinear.Linear.41.bias        |     1      |\n",
      "|        nlinear.Linear.42.weight       |     40     |\n",
      "|         nlinear.Linear.42.bias        |     1      |\n",
      "|        nlinear.Linear.43.weight       |     40     |\n",
      "|         nlinear.Linear.43.bias        |     1      |\n",
      "|        nlinear.Linear.44.weight       |     40     |\n",
      "|         nlinear.Linear.44.bias        |     1      |\n",
      "|        nlinear.Linear.45.weight       |     40     |\n",
      "|         nlinear.Linear.45.bias        |     1      |\n",
      "|        nlinear.Linear.46.weight       |     40     |\n",
      "|         nlinear.Linear.46.bias        |     1      |\n",
      "|        nlinear.Linear.47.weight       |     40     |\n",
      "|         nlinear.Linear.47.bias        |     1      |\n",
      "|        nlinear.Linear.48.weight       |     40     |\n",
      "|         nlinear.Linear.48.bias        |     1      |\n",
      "|        nlinear.Linear.49.weight       |     40     |\n",
      "|         nlinear.Linear.49.bias        |     1      |\n",
      "|        nlinear.Linear.50.weight       |     40     |\n",
      "|         nlinear.Linear.50.bias        |     1      |\n",
      "|        nlinear.Linear.51.weight       |     40     |\n",
      "|         nlinear.Linear.51.bias        |     1      |\n",
      "|        nlinear.Linear.52.weight       |     40     |\n",
      "|         nlinear.Linear.52.bias        |     1      |\n",
      "|        nlinear.Linear.53.weight       |     40     |\n",
      "|         nlinear.Linear.53.bias        |     1      |\n",
      "|        nlinear.Linear.54.weight       |     40     |\n",
      "|         nlinear.Linear.54.bias        |     1      |\n",
      "|        nlinear.Linear.55.weight       |     40     |\n",
      "|         nlinear.Linear.55.bias        |     1      |\n",
      "|        nlinear.Linear.56.weight       |     40     |\n",
      "|         nlinear.Linear.56.bias        |     1      |\n",
      "|        nlinear.Linear.57.weight       |     40     |\n",
      "|         nlinear.Linear.57.bias        |     1      |\n",
      "|        nlinear.Linear.58.weight       |     40     |\n",
      "|         nlinear.Linear.58.bias        |     1      |\n",
      "|        nlinear.Linear.59.weight       |     40     |\n",
      "|         nlinear.Linear.59.bias        |     1      |\n",
      "|        nlinear.Linear.60.weight       |     40     |\n",
      "|         nlinear.Linear.60.bias        |     1      |\n",
      "|        nlinear.Linear.61.weight       |     40     |\n",
      "|         nlinear.Linear.61.bias        |     1      |\n",
      "|        nlinear.Linear.62.weight       |     40     |\n",
      "|         nlinear.Linear.62.bias        |     1      |\n",
      "|        nlinear.Linear.63.weight       |     40     |\n",
      "|         nlinear.Linear.63.bias        |     1      |\n",
      "|        nlinear.Linear.64.weight       |     40     |\n",
      "|         nlinear.Linear.64.bias        |     1      |\n",
      "|        nlinear.Linear.65.weight       |     40     |\n",
      "|         nlinear.Linear.65.bias        |     1      |\n",
      "|        nlinear.Linear.66.weight       |     40     |\n",
      "|         nlinear.Linear.66.bias        |     1      |\n",
      "|        nlinear.Linear.67.weight       |     40     |\n",
      "|         nlinear.Linear.67.bias        |     1      |\n",
      "|        nlinear.Linear.68.weight       |     40     |\n",
      "|         nlinear.Linear.68.bias        |     1      |\n",
      "|        nlinear.Linear.69.weight       |     40     |\n",
      "|         nlinear.Linear.69.bias        |     1      |\n",
      "|  gnn.model.convs.0.message_lin.weight |    4544    |\n",
      "|   gnn.model.convs.0.message_lin.bias  |     64     |\n",
      "|    gnn.model.convs.0.agg_lin.weight   |    8576    |\n",
      "|     gnn.model.convs.0.agg_lin.bias    |     64     |\n",
      "|   gnn.model.node_post_mlp.0.0.weight  |    4096    |\n",
      "|    gnn.model.node_post_mlp.0.0.bias   |     64     |\n",
      "|    gnn.model.node_post_mlp.1.weight   |    4096    |\n",
      "|     gnn.model.node_post_mlp.1.bias    |     64     |\n",
      "| gnn.model.edge_update_mlps.0.0.weight |    8256    |\n",
      "|  gnn.model.edge_update_mlps.0.0.bias  |     64     |\n",
      "|  gnn.model.sage.0.message_lin.weight  |    4096    |\n",
      "|   gnn.model.sage.0.message_lin.bias   |     64     |\n",
      "|    gnn.model.sage.0.agg_lin.weight    |    8192    |\n",
      "|     gnn.model.sage.0.agg_lin.bias     |     64     |\n",
      "|       gnn.mlp.layers.0.0.weight       |    8192    |\n",
      "|        gnn.mlp.layers.0.0.bias        |     64     |\n",
      "|       gnn.mlp.layers.1.0.weight       |     64     |\n",
      "|        gnn.mlp.layers.1.0.bias        |     1      |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 53495\n",
      "model parameters: 53495\n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:03<00:00, 339.49it/s, epoch=0, lr=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 3.9060938358306885\n",
      "Traininng loss : 3.9088727150644575\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:01<00:00, 191.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.2112644463777542, 'mae': 0.9795727729797363, 'mse': 1.9549514055252075, 'r2': -1.1239018440246582, 'r2_weighted': -1.1733860969543457}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:01<00:00, 84.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.637043833732605, 'mae': 0.6853190064430237, 'mse': 1.187572956085205, 'r2': -1.3775568008422852, 'r2_weighted': -1.1155619621276855}\n",
      "Validation loss decreased (inf --> 1.954951).  Saving model ...\n",
      "Saving run checkpoint to './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:04<00:00, 303.11it/s, epoch=1, lr=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 11.686584711074829\n",
      "Traininng loss : 2.8653311218534196\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:01<00:00, 222.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.2322622835636139, 'mae': 1.0720124244689941, 'mse': 2.0885109901428223, 'r2': -1.3258262872695923, 'r2_weighted': -1.321868658065796}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:01<00:00, 88.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.6503279209136963, 'mae': 0.7866218090057373, 'mse': 1.3099416494369507, 'r2': -1.9728169441223145, 'r2_weighted': -1.3335520029067993}\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Saving run checkpoint to './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:04<00:00, 302.77it/s, epoch=2, lr=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 cost time: 19.154665231704712\n",
      "Traininng loss : 2.368828352008547\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:01<00:00, 191.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.2376275509595871, 'mae': 1.0838490724563599, 'mse': 2.0832481384277344, 'r2': -1.3197124004364014, 'r2_weighted': -1.316017746925354}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:01<00:00, 83.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.6668259501457214, 'mae': 0.8071932196617126, 'mse': 1.3090503215789795, 'r2': -2.1144371032714844, 'r2_weighted': -1.3319640159606934}\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Saving run checkpoint to './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:04<00:00, 328.77it/s, epoch=3, lr=0.000299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 26.622079849243164\n",
      "Traininng loss : 2.1463338619186763\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:01<00:00, 227.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.23966792225837708, 'mae': 1.0535560846328735, 'mse': 1.9908502101898193, 'r2': -1.2016985416412354, 'r2_weighted': -1.2132960557937622}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:01<00:00, 121.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.6830009818077087, 'mae': 0.7802180647850037, 'mse': 1.2405481338500977, 'r2': -1.8966426849365234, 'r2_weighted': -1.2099332809448242}\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Saving run checkpoint to './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:04<00:00, 302.90it/s, epoch=4, lr=0.000299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost time: 33.64450645446777\n",
      "Traininng loss : 2.0342543848923276\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:01<00:00, 196.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.2412334829568863, 'mae': 1.0363191366195679, 'mse': 1.9437885284423828, 'r2': -1.1415396928787231, 'r2_weighted': -1.1609759330749512}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:01<00:00, 92.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.6939830780029297, 'mae': 0.7668523788452148, 'mse': 1.206560730934143, 'r2': -1.798567295074463, 'r2_weighted': -1.1493875980377197}\n",
      "Validation loss decreased (1.954951 --> 1.943789).  Saving model ...\n",
      "Saving run checkpoint to './results/runs/SP500/w40h24s1/0b6d5b1f3438b07d3c634f40b4938b71'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1280/1325 [00:03<00:00, 465.96it/s, epoch=5, lr=0.000298]"
     ]
    }
   ],
   "source": [
    "exp = NlinearGNNEnhanceExperiment(\n",
    "    dataset_type=\"SP500\",\n",
    "    horizon=24,\n",
    "    pred_len=1,\n",
    "    data_path='/notebooks/pytorch_timeseries/data/',\n",
    "    windows=40,\n",
    "    \n",
    ")\n",
    "\n",
    "exp.run(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNPredictor(\n",
       "  (model): GNNStack(\n",
       "    (convs): ModuleList(\n",
       "      (0): EGraphSage(10, 64)\n",
       "      (1-2): 2 x EGraphSage(64, 64)\n",
       "    )\n",
       "    (node_post_mlp): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): Linear(in_features=64, out_features=192, bias=True)\n",
       "    )\n",
       "    (edge_update_mlps): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=129, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1-2): 2 x Sequential(\n",
       "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (sage): ModuleList(\n",
       "      (0): PGraphSage(64, 64)\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLPNet(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/notebooks/pytorch_timeseries/notebooks/trials/201_GP.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B250-internal-yww-jupyter/notebooks/pytorch_timeseries/notebooks/trials/201_GP.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m hot_encoding_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(feature_number)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B250-internal-yww-jupyter/notebooks/pytorch_timeseries/notebooks/trials/201_GP.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m temporal_encoding_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, output_length) \u001b[39m/\u001b[39m output_length \u001b[39m# \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B250-internal-yww-jupyter/notebooks/pytorch_timeseries/notebooks/trials/201_GP.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m x_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mconcat([hot_encoding_matrix, temporal_encoding_matrix], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "feature_number = 10\n",
    "output_length = 10\n",
    "hot_encoding_matrix = torch.eye(feature_number).detach()\n",
    "temporal_encoding_matrix = torch.arange(0, output_length) / output_length # \n",
    "\n",
    "\n",
    "x_input = torch.concat([hot_encoding_matrix, temporal_encoding_matrix], dim=0).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_encoding_matrix.unsqueeze(1).repeat(1, feature_number).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
