{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import signal\n",
    "import threading\n",
    "import time\n",
    "import hashlib\n",
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "####\n",
    "from typing import Dict, List, Type, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import MeanSquaredError, MetricCollection, MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch_timeseries.data.scaler import *\n",
    "from torch_timeseries.datasets import *\n",
    "from torch_timeseries.experiments.experiment import Experiment\n",
    "\n",
    "from torch_timeseries.datasets.dataset import TimeSeriesDataset\n",
    "from torch_timeseries.datasets.splitter import SequenceRandomSplitter, SequenceSplitter\n",
    "from torch_timeseries.datasets.dataloader import (\n",
    "    ChunkSequenceTimefeatureDataLoader,\n",
    "    DDPChunkSequenceTimefeatureDataLoader,\n",
    ")\n",
    "from torch_timeseries.datasets.wrapper import MultiStepTimeFeatureSet\n",
    "from torch_timeseries.models.Informer import Informer\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, Subset\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn as nn\n",
    "from dataclasses import asdict,dataclass\n",
    "\n",
    "from torch_timeseries.nn.metric import R2, Corr, TrendAcc,RMSE, compute_corr, compute_r2\n",
    "from torch_timeseries.metrics.masked_mape import MaskedMAPE\n",
    "from torch_timeseries.utils.early_stopping import EarlyStopping\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "\n",
    "from torch_timeseries.layers.tcn_output8 import TCNOuputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'device'\n",
    "\n",
    "# dataset : TimeSeriesDataset = ExchangeRate(root='/notebooks/pytorch_timeseries/')\n",
    "# scaler = StandarScaler(device='')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch_timeseries.datasets.dataset import TimeSeriesStaticGraphDataset\n",
    "\n",
    "\n",
    "class STI(torch.nn.Module):\n",
    "    # \n",
    "    def __init__(self,seq_len, latent_dim,num_nodes,out_seq_len,tcn_layers=5,dilated_factor=2,tcn_channel=16,kernel_set=[2,3,6,7],d0=1, layer_norm_affline=True) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tcn = TCNOuputLayer(\n",
    "                        seq_len,num_nodes,out_seq_len,\n",
    "                        tcn_layers,3,dilated_factor,\n",
    "                        tcn_channel,kernel_set=kernel_set,d0=d0,\n",
    "                        layer_norm_affline=True     \n",
    "                    )\n",
    "\n",
    "        self.tcn_input_dim = self.tcn.tcn.receptive_field\n",
    "        self.tcn = TCNOuputLayer(\n",
    "                        self.tcn_input_dim,num_nodes,out_seq_len,\n",
    "                        tcn_layers,3,dilated_factor,\n",
    "                        tcn_channel,kernel_set=kernel_set,d0=d0,\n",
    "                        layer_norm_affline=True     \n",
    "                    )\n",
    "\n",
    "\n",
    "        self.spatial_projection = nn.Sequential(\n",
    "            nn.Linear(seq_len, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        self.temporal_projection = nn.Sequential(\n",
    "            nn.Linear(num_nodes, self.tcn_input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.tcn_input_dim, self.tcn_input_dim)\n",
    "        )\n",
    "        \n",
    "        self.freq_projection = nn.Sequential(\n",
    "            nn.Linear(num_nodes, self.tcn_input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.tcn_input_dim, self.tcn_input_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        self.spatial_rebuild = nn.Sequential(\n",
    "            nn.Linear(latent_dim, self.tcn_input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.tcn_input_dim, self.tcn_input_dim)\n",
    "        )\n",
    "        \n",
    "        self.temporal_rebuild = nn.Sequential(\n",
    "            nn.Linear(seq_len, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, num_nodes)\n",
    "        )\n",
    "        \n",
    "        self.freq_rebuild = nn.Sequential(\n",
    "            nn.Linear(seq_len, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, num_nodes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x : (B, N, T)\n",
    "        \n",
    "        seq_last = x[:,:,-1:].detach()\n",
    "        x = x - seq_last\n",
    "        # import pdb;pdb.set_trace\n",
    "        \n",
    "        \n",
    "        Xs = self.spatial_projection(x) # (B, N, latent_dim)\n",
    "        Xt = self.temporal_projection(x.transpose(1, 2)) # (B, T, tcn_input_dim)\n",
    "        Xf = torch.abs(torch.fft.fft(x, dim=2, norm='forward'))# (B, N, T)\n",
    "        \n",
    "        Xf = self.freq_projection(Xf.transpose(1,2 ))  # （B, T, latent_dim)\n",
    "        \n",
    "        Zs = self.spatial_rebuild(Xs)  # (B, N, tcn_input_dim)\n",
    "        Zt = self.temporal_rebuild(Xt.transpose(1,2)).transpose(1,2) # (B, N, tcn_input_dim)\n",
    "        Zf = self.freq_rebuild(Xf.transpose(1, 2)).transpose(1, 2) # (B, N, tcn_input_dim)\n",
    "        \n",
    "        Z = torch.stack([Zs, Zt, Zf], dim=1)\n",
    "        O = self.tcn(Z) # (B, O , N)\n",
    "        \n",
    "        \n",
    "        O = (O.transpose(1,2) + seq_last).transpose(1,2)\n",
    "        return O\n",
    "        # Zs = self.freq_projection(Xs)\n",
    "        # Zt = self.freq_projection(Xt)\n",
    "        # Zf = self.freq_projection(Xf)\n",
    "    # self.feq_projection = nn.Sequential(\n",
    "    #     nn.Linear()\n",
    "    # )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class TSExperiment(Experiment):\n",
    "    \n",
    "    latent_dim: int = 1024\n",
    "    tcn_layers : int = 5\n",
    "\n",
    "    def _process_one_batch(self, batch_x, batch_y, batch_x_date_enc, batch_y_date_enc):\n",
    "        # inputs:\n",
    "        # batch_x: (B, T, N)\n",
    "        # batch_y: (B, O, N)\n",
    "        # ouputs:\n",
    "        # - pred: (B, N)/(B, O, N)\n",
    "        # - label: (B, N)/(B, O, N)\n",
    "        batch_size = batch_x.size(0)\n",
    "        batch_x = batch_x.to(self.device, dtype=torch.float32)\n",
    "        batch_y = batch_y.to(self.device, dtype=torch.float32)\n",
    "        batch_x_date_enc = batch_x_date_enc.to(self.device).float()\n",
    "        batch_y_date_enc = batch_y_date_enc.to(self.device).float()\n",
    "        batch_x = batch_x.transpose(1,2)\n",
    "        outputs = self.model(batch_x)  # torch.Size([batch_size, num_nodes])\n",
    "        # single step prediction\n",
    "        return outputs, batch_y\n",
    "\n",
    "\n",
    "    def _init_model(self):\n",
    "        predefined_NN_adj = None\n",
    "        padded_A = None\n",
    "        if isinstance(self.dataset, TimeSeriesStaticGraphDataset) and self.pred_len > 1:\n",
    "            predefined_NN_adj = torch.tensor(self.dataset.adj).to(self.device)\n",
    "            D = torch.diag(torch.sum(predefined_NN_adj, dim=1))\n",
    "            D_sqrt_inv = torch.sqrt(torch.inverse(D))\n",
    "            normalized_predefined_adj = D_sqrt_inv @predefined_NN_adj @ D_sqrt_inv\n",
    "            padded_A = torch.nn.functional.pad(normalized_predefined_adj, (0, self.windows, 0, self.windows), mode='constant', value=0).float()\n",
    "\n",
    "        else:\n",
    "            padded_A = None\n",
    "\n",
    "        if isinstance(self.dataset, PeMS_D7):\n",
    "            temporal_embed_dim = 0\n",
    "        else:\n",
    "            temporal_embed_dim = 4\n",
    "        self.model = STI(\n",
    "            tcn_layers=self.tcn_layers,\n",
    "            seq_len=self.windows, latent_dim=self.latent_dim,num_nodes=self.dataset.num_features,out_seq_len=self.pred_len,dilated_factor=2,tcn_channel=16,kernel_set=[2,3,6,7],d0=1, layer_norm_affline=True\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /notebooks/pytorch_timeseries/data/solar_AL/solar_AL.txt.gz\n",
      "Extracting /notebooks/pytorch_timeseries/data/solar_AL/solar_AL.txt.gz to /notebooks/pytorch_timeseries/data/solar_AL\n",
      "train steps: 36601\n",
      "val steps: 10321\n",
      "test steps: 5065\n",
      "torch.get_default_dtype() torch.float32\n",
      "Creating running results saving dir: './results/runs/SolarEnergy/w168h24s1/b2324a7b7ac74be9469227cc8ea12a11'.\n",
      "run : 0 in seed: 662\n",
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|        tcn.channel_layer.weight       |     48     |\n",
      "|         tcn.channel_layer.bias        |     16     |\n",
      "| tcn.tcn.filter_convs.0.tconv.0.weight |    128     |\n",
      "|  tcn.tcn.filter_convs.0.tconv.0.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.0.tconv.1.weight |    192     |\n",
      "|  tcn.tcn.filter_convs.0.tconv.1.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.0.tconv.2.weight |    384     |\n",
      "|  tcn.tcn.filter_convs.0.tconv.2.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.0.tconv.3.weight |    448     |\n",
      "|  tcn.tcn.filter_convs.0.tconv.3.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.1.tconv.0.weight |    128     |\n",
      "|  tcn.tcn.filter_convs.1.tconv.0.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.1.tconv.1.weight |    192     |\n",
      "|  tcn.tcn.filter_convs.1.tconv.1.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.1.tconv.2.weight |    384     |\n",
      "|  tcn.tcn.filter_convs.1.tconv.2.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.1.tconv.3.weight |    448     |\n",
      "|  tcn.tcn.filter_convs.1.tconv.3.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.2.tconv.0.weight |    128     |\n",
      "|  tcn.tcn.filter_convs.2.tconv.0.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.2.tconv.1.weight |    192     |\n",
      "|  tcn.tcn.filter_convs.2.tconv.1.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.2.tconv.2.weight |    384     |\n",
      "|  tcn.tcn.filter_convs.2.tconv.2.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.2.tconv.3.weight |    448     |\n",
      "|  tcn.tcn.filter_convs.2.tconv.3.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.3.tconv.0.weight |    128     |\n",
      "|  tcn.tcn.filter_convs.3.tconv.0.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.3.tconv.1.weight |    192     |\n",
      "|  tcn.tcn.filter_convs.3.tconv.1.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.3.tconv.2.weight |    384     |\n",
      "|  tcn.tcn.filter_convs.3.tconv.2.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.3.tconv.3.weight |    448     |\n",
      "|  tcn.tcn.filter_convs.3.tconv.3.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.4.tconv.0.weight |    128     |\n",
      "|  tcn.tcn.filter_convs.4.tconv.0.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.4.tconv.1.weight |    192     |\n",
      "|  tcn.tcn.filter_convs.4.tconv.1.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.4.tconv.2.weight |    384     |\n",
      "|  tcn.tcn.filter_convs.4.tconv.2.bias  |     4      |\n",
      "| tcn.tcn.filter_convs.4.tconv.3.weight |    448     |\n",
      "|  tcn.tcn.filter_convs.4.tconv.3.bias  |     4      |\n",
      "|  tcn.tcn.gate_convs.0.tconv.0.weight  |    128     |\n",
      "|   tcn.tcn.gate_convs.0.tconv.0.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.0.tconv.1.weight  |    192     |\n",
      "|   tcn.tcn.gate_convs.0.tconv.1.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.0.tconv.2.weight  |    384     |\n",
      "|   tcn.tcn.gate_convs.0.tconv.2.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.0.tconv.3.weight  |    448     |\n",
      "|   tcn.tcn.gate_convs.0.tconv.3.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.1.tconv.0.weight  |    128     |\n",
      "|   tcn.tcn.gate_convs.1.tconv.0.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.1.tconv.1.weight  |    192     |\n",
      "|   tcn.tcn.gate_convs.1.tconv.1.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.1.tconv.2.weight  |    384     |\n",
      "|   tcn.tcn.gate_convs.1.tconv.2.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.1.tconv.3.weight  |    448     |\n",
      "|   tcn.tcn.gate_convs.1.tconv.3.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.2.tconv.0.weight  |    128     |\n",
      "|   tcn.tcn.gate_convs.2.tconv.0.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.2.tconv.1.weight  |    192     |\n",
      "|   tcn.tcn.gate_convs.2.tconv.1.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.2.tconv.2.weight  |    384     |\n",
      "|   tcn.tcn.gate_convs.2.tconv.2.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.2.tconv.3.weight  |    448     |\n",
      "|   tcn.tcn.gate_convs.2.tconv.3.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.3.tconv.0.weight  |    128     |\n",
      "|   tcn.tcn.gate_convs.3.tconv.0.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.3.tconv.1.weight  |    192     |\n",
      "|   tcn.tcn.gate_convs.3.tconv.1.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.3.tconv.2.weight  |    384     |\n",
      "|   tcn.tcn.gate_convs.3.tconv.2.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.3.tconv.3.weight  |    448     |\n",
      "|   tcn.tcn.gate_convs.3.tconv.3.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.4.tconv.0.weight  |    128     |\n",
      "|   tcn.tcn.gate_convs.4.tconv.0.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.4.tconv.1.weight  |    192     |\n",
      "|   tcn.tcn.gate_convs.4.tconv.1.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.4.tconv.2.weight  |    384     |\n",
      "|   tcn.tcn.gate_convs.4.tconv.2.bias   |     4      |\n",
      "|  tcn.tcn.gate_convs.4.tconv.3.weight  |    448     |\n",
      "|   tcn.tcn.gate_convs.4.tconv.3.bias   |     4      |\n",
      "|    tcn.tcn.residual_convs.0.weight    |    256     |\n",
      "|     tcn.tcn.residual_convs.0.bias     |     16     |\n",
      "|    tcn.tcn.residual_convs.1.weight    |    256     |\n",
      "|     tcn.tcn.residual_convs.1.bias     |     16     |\n",
      "|    tcn.tcn.residual_convs.2.weight    |    256     |\n",
      "|     tcn.tcn.residual_convs.2.bias     |     16     |\n",
      "|    tcn.tcn.residual_convs.3.weight    |    256     |\n",
      "|     tcn.tcn.residual_convs.3.bias     |     16     |\n",
      "|    tcn.tcn.residual_convs.4.weight    |    256     |\n",
      "|     tcn.tcn.residual_convs.4.bias     |     16     |\n",
      "|      tcn.tcn.skip_convs.0.weight      |   46336    |\n",
      "|       tcn.tcn.skip_convs.0.bias       |     16     |\n",
      "|      tcn.tcn.skip_convs.1.weight      |   43264    |\n",
      "|       tcn.tcn.skip_convs.1.bias       |     16     |\n",
      "|      tcn.tcn.skip_convs.2.weight      |   37120    |\n",
      "|       tcn.tcn.skip_convs.2.bias       |     16     |\n",
      "|      tcn.tcn.skip_convs.3.weight      |   24832    |\n",
      "|       tcn.tcn.skip_convs.3.bias       |     16     |\n",
      "|      tcn.tcn.skip_convs.4.weight      |    256     |\n",
      "|       tcn.tcn.skip_convs.4.bias       |     16     |\n",
      "|         tcn.tcn.norms.0.weight        |   396752   |\n",
      "|          tcn.tcn.norms.0.bias         |   396752   |\n",
      "|         tcn.tcn.norms.1.weight        |   370448   |\n",
      "|          tcn.tcn.norms.1.bias         |   370448   |\n",
      "|         tcn.tcn.norms.2.weight        |   317840   |\n",
      "|          tcn.tcn.norms.2.bias         |   317840   |\n",
      "|         tcn.tcn.norms.3.weight        |   212624   |\n",
      "|          tcn.tcn.norms.3.bias         |   212624   |\n",
      "|         tcn.tcn.norms.4.weight        |    2192    |\n",
      "|          tcn.tcn.norms.4.bias         |    2192    |\n",
      "|          tcn.tcn.skip0.weight         |   47872    |\n",
      "|           tcn.tcn.skip0.bias          |     16     |\n",
      "|          tcn.tcn.skipE.weight         |    256     |\n",
      "|           tcn.tcn.skipE.bias          |     16     |\n",
      "|        tcn.tcn.end_conv.weight        |    256     |\n",
      "|         tcn.tcn.end_conv.bias         |     16     |\n",
      "|          tcn.end_layer.weight         |     16     |\n",
      "|           tcn.end_layer.bias          |     1      |\n",
      "|      spatial_projection.0.weight      |    2688    |\n",
      "|       spatial_projection.0.bias       |     16     |\n",
      "|      spatial_projection.2.weight      |    256     |\n",
      "|       spatial_projection.2.bias       |     16     |\n",
      "|      temporal_projection.0.weight     |   25619    |\n",
      "|       temporal_projection.0.bias      |    187     |\n",
      "|      temporal_projection.2.weight     |   34969    |\n",
      "|       temporal_projection.2.bias      |    187     |\n",
      "|        freq_projection.0.weight       |   25619    |\n",
      "|         freq_projection.0.bias        |    187     |\n",
      "|        freq_projection.2.weight       |   34969    |\n",
      "|         freq_projection.2.bias        |    187     |\n",
      "|        spatial_rebuild.0.weight       |    2992    |\n",
      "|         spatial_rebuild.0.bias        |    187     |\n",
      "|        spatial_rebuild.2.weight       |   34969    |\n",
      "|         spatial_rebuild.2.bias        |    187     |\n",
      "|       temporal_rebuild.0.weight       |    2688    |\n",
      "|        temporal_rebuild.0.bias        |     16     |\n",
      "|       temporal_rebuild.2.weight       |    2192    |\n",
      "|        temporal_rebuild.2.bias        |    137     |\n",
      "|         freq_rebuild.0.weight         |    2688    |\n",
      "|          freq_rebuild.0.bias          |     16     |\n",
      "|         freq_rebuild.2.weight         |    2192    |\n",
      "|          freq_rebuild.2.bias          |    137     |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 2986454\n",
      "model parameters: 2986454\n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36601/36601 [02:41<00:00, 226.06it/s, epoch=0, loss=0.108, lr=0.0005] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 161.91121077537537\n",
      "Val on train....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36601/36601 [00:53<00:00, 688.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val on train result: {'corr': 0.39743703603744507, 'mae': 0.24525612592697144, 'mse': 0.16543780267238617, 'r2': 0.8433681130409241, 'r2_weighted': 0.8433005809783936}\n",
      "Evaluating .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10321/10321 [00:16<00:00, 644.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_results: {'corr': 0.43304458260536194, 'mae': 0.2397182732820511, 'mse': 0.15938986837863922, 'r2': 0.8301283717155457, 'r2_weighted': 0.8306878805160522}\n",
      "Testing .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5065/5065 [00:09<00:00, 516.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results: {'corr': 0.4295472204685211, 'mae': 0.25305303931236267, 'mse': 0.1654668003320694, 'r2': 0.7468786239624023, 'r2_weighted': 0.7499808073043823}\n",
      "Validation loss decreased (inf --> 0.159390).  Saving model ...\n",
      "Saving run checkpoint to './results/runs/SolarEnergy/w168h24s1/b2324a7b7ac74be9469227cc8ea12a11'.\n",
      "Run state saved ... \n",
      "torch.get_default_dtype() torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36601/36601 [02:48<00:00, 217.00it/s, epoch=1, loss=0.105, lr=0.0005] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 409.89287209510803\n",
      "Val on train....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8064/36601 [00:13<00:44, 638.28it/s]"
     ]
    }
   ],
   "source": [
    "exp = TSExperiment(\n",
    "    device=\"cuda:0\",\n",
    "    latent_dim=16,\n",
    "    dataset_type=\"SolarEnergy\",\n",
    "    tcn_layers=5,\n",
    "    horizon=24,\n",
    "    epochs=100,\n",
    "    windows=168,\n",
    "    lr=0.0001,\n",
    "    data_path='/notebooks/pytorch_timeseries/data/'\n",
    ")\n",
    "# model = STI(168, 128, 9, 1)\n",
    "# out = model(data)\n",
    "\n",
    "exp.run(seed=662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xf = torch.fft.fft(data, dim=2, norm='forward')[:, :, :]\n",
    "amp = torch.abs(Xf)\n",
    "amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3273,  0.1516,  0.0161, -0.8483,  0.0766,  0.7280,  0.6113, -0.0470,\n",
      "         0.9393,  0.6909,  0.0947,  0.8419,  0.1639, -0.1819,  1.0376,  0.0166,\n",
      "        -0.7076,  0.9921,  0.2978, -0.4104, -1.0616,  0.6393,  0.1030,  0.5463,\n",
      "        -0.3269,  0.1066, -0.1008,  0.2158, -0.1032, -0.3987, -0.4024, -0.3123],\n",
      "       grad_fn=<SelectBackward0>) tensor([-0.4796, -0.6486, -1.6920, -1.0470, -0.4733, -0.2575, -0.1732, -0.4943,\n",
      "         0.1441, -0.6750,  0.2937, -0.0437, -0.0987,  0.2391,  0.2676, -0.3876,\n",
      "         0.9132,  0.7786, -0.3593, -0.1810, -1.0035, -1.1220, -0.5471,  0.9383,\n",
      "        -0.9157,  0.2677, -0.0436,  0.3239,  0.2225,  0.7437,  0.0334,  0.9020],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([7, 32]) torch.Size([7, 32])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4796, -0.6486, -1.6920, -1.0470, -0.4733, -0.2575, -0.1732, -0.4943,\n",
       "         0.1441, -0.6750,  0.2937, -0.0437, -0.0987,  0.2391,  0.2676, -0.3876,\n",
       "         0.9132,  0.7786, -0.3593, -0.1810, -1.0035, -1.1220, -0.5471,  0.9383,\n",
       "        -0.9157,  0.2677, -0.0436,  0.3239,  0.2225,  0.7437,  0.0334,  0.9020],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi = 0\n",
    "\n",
    "edge_nt = torch.stack((\n",
    "    edge_index[bi][0][edge_index[bi][0] < self.node_num], # source\n",
    "    edge_index[bi][1][edge_index[bi][1] >= self.node_num] # target\n",
    "    ))\n",
    "edge_tn = torch.stack((\n",
    "    edge_index[bi][0][edge_index[bi][0] >= self.node_num],\n",
    "    edge_index[bi][1][edge_index[bi][1] < self.node_num]\n",
    "    ))               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
