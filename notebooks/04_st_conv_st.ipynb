{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from class_resolver.contrib.torch import activation_resolver\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn import FAConv,HeteroConv\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "# from torch_timeseries.layers.graphsage import MyGraphSage\n",
    "\n",
    "# from torch_timeseries.utils.norm import hetero_directed_norm\n",
    "\n",
    "def hetero_directed_norm(edge_index, edge_weight=None, num_nodes=None,\n",
    "              dtype=None):\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        raise NotImplementedError(\"Operation of Sparse Tensor Not defined!\")\n",
    "    else:\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        \n",
    "        # in degree of every node |N|\n",
    "        in_deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "\n",
    "        # out degree of every node |N|\n",
    "        out_deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "\n",
    "        # nomalization\n",
    "        in_deg_inv_sqrt = in_deg.pow_(-0.5)\n",
    "        out_deg_inv_sqrt = out_deg.pow_(-0.5)\n",
    "        in_deg_inv_sqrt.masked_fill_(in_deg_inv_sqrt == float('inf'), 0)\n",
    "        out_deg_inv_sqrt.masked_fill_(out_deg_inv_sqrt == float('inf'), 0)\n",
    "\n",
    "        # source node out degree, target node in degree \n",
    "        return out_deg_inv_sqrt[row] * edge_weight * in_deg_inv_sqrt[col]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TSConv(MessagePassing):\n",
    "    # convolution for relation < t -> s >\n",
    "    def __init__(self, in_channels, out_channels, eps=0.9):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.eps = eps\n",
    "        self.att_g = Linear(2*in_channels,1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.att_g.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N+T, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        # edge_index has shape [E, weight_dim]\n",
    "        \n",
    "        xt = x[0]\n",
    "        xs = x[1]\n",
    "        \n",
    "        x = torch.concat([xs, xt], dim=0)\n",
    "        \n",
    "        edge_weight = hetero_directed_norm(  # yapf: disable\n",
    "            edge_index, edge_weight, x.size(self.node_dim), dtype=x.dtype)\n",
    "\n",
    "        t2s_info = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "        \n",
    "        return t2s_info\n",
    "\n",
    "    def message(self,x_i, x_j, edge_weight):\n",
    "        # x_j has shape [|E|, out_channels] , The first n edges are edges of  spatial nodes\n",
    "        # x_j denotes a lifted tensor, which contains the source node features of each edge, source_node(如果flow 是 source_to_target)\n",
    "        # 要从从有向图的角度来解释 edge_index 有几个，就有几个x_\n",
    "        \n",
    "        # 对 st , ts分别定义\n",
    "        \n",
    "        alpha_i_j = self.att_g(torch.concat([x_i, x_j], axis=1)).tanh().squeeze(-1) # ( |E|, )    \n",
    "        \n",
    "        return x_j *( alpha_i_j * edge_weight ).view(-1,1)\n",
    "\n",
    "\n",
    "class STConv(MessagePassing):\n",
    "    # convolution for relation < s -> t >\n",
    "    def __init__(self, in_channels, out_channels, eps=0.9):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.eps = eps\n",
    "        self.att_g = Linear(2*in_channels,1, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.att_g.reset_parameters()\n",
    "        # self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N+T, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        # edge_index has shape [E, weight_dim]\n",
    "        xs = x[0]\n",
    "        xt = x[1]\n",
    "        \n",
    "        x = torch.concat([xs, xt], dim=0)\n",
    "        \n",
    "        edge_weight = hetero_directed_norm(  # yapf: disable\n",
    "            edge_index, edge_weight, x.size(self.node_dim), dtype=x.dtype)\n",
    "        s2t_info = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "        return s2t_info\n",
    "\n",
    "    def message(self,x_i, x_j, edge_weight):\n",
    "        # x_j has shape [|E|, out_channels] , The first n edges are edges of  spatial nodes\n",
    "        # x_j denotes a lifted tensor, which contains the source node features of each edge, source_node(如果flow 是 source_to_target)\n",
    "        # 要从从有向图的角度来解释 edge_index 有几个，就有几个x_\n",
    "        \n",
    "        # 对 st , ts分别定义\n",
    "        alpha_i_j = self.att_g(torch.concat([x_i, x_j], axis=1)).tanh().squeeze(-1) # ( |E|, )    \n",
    "        \n",
    "        return x_j *( alpha_i_j * edge_weight ).view(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "class SSConv(MessagePassing):\n",
    "    # convolution for relation < s -> t >\n",
    "    def __init__(self, in_channels, out_channels,add_self_loops=True, eps=0.9):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.eps = eps\n",
    "        self.att_g = Linear(2*in_channels,1, bias=False)\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.att_g.reset_parameters()\n",
    "        # self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N+T, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        # edge_index has shape [E, weight_dim]\n",
    "        edge_index , edge_weight = gcn_norm(  # yapf: disable\n",
    "            edge_index, edge_weight,add_self_loops=self.add_self_loops,num_nodes=x.size(self.node_dim), dtype=x.dtype)\n",
    "        s2s_info = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "        return s2s_info\n",
    "\n",
    "    def message(self,x_i, x_j, edge_weight):\n",
    "        # x_j has shape [|E|, out_channels] , The first n edges are edges of  spatial nodes\n",
    "        # x_j denotes a lifted tensor, which contains the source node features of each edge, source_node(如果flow 是 source_to_target)\n",
    "        # 要从从有向图的角度来解释 edge_index 有几个，就有几个x_\n",
    "        \n",
    "        # 对 st , ts分别定义\n",
    "        alpha_i_j = self.att_g(torch.concat([x_i, x_j], axis=1)).tanh().squeeze(-1) # ( |E|, )    \n",
    "        \n",
    "        return x_j *( alpha_i_j * edge_weight ).view(-1,1)\n",
    "\n",
    "\n",
    "class TTConv(MessagePassing):\n",
    "    # convolution for relation < s -> t >\n",
    "    def __init__(self, in_channels, out_channels,add_self_loops=True, eps=0.9):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.eps = eps\n",
    "        self.att_g = Linear(2*in_channels,1, bias=False)\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.att_g.reset_parameters()\n",
    "        # self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N+T, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        # edge_index has shape [E, weight_dim]\n",
    "        \n",
    "        edge_index , edge_weight = gcn_norm(  # yapf: disable\n",
    "            edge_index, edge_weight,add_self_loops=self.add_self_loops,num_nodes=x.size(self.node_dim), dtype=x.dtype)\n",
    "        t2t_info = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "        return t2t_info\n",
    "\n",
    "    def message(self,x_i, x_j, edge_weight):\n",
    "        # x_j has shape [|E|, out_channels] , The first n edges are edges of  spatial nodes\n",
    "        # x_j denotes a lifted tensor, which contains the source node features of each edge, source_node(如果flow 是 source_to_target)\n",
    "        # 要从从有向图的角度来解释 edge_index 有几个，就有几个x_\n",
    "        \n",
    "        # 对 st , ts分别定义\n",
    "        alpha_i_j = self.att_g(torch.concat([x_i, x_j], axis=1)).tanh().squeeze(-1) # ( |E|, )    \n",
    "        \n",
    "        return x_j *( alpha_i_j * edge_weight ).view(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class HeteroFASTGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,node_num,seq_len, in_channels, hidden_channels, n_layers, out_channels=None,\n",
    "        dropout=0, norm=None, act='relu',n_first=True, act_first=False, eps=0.9, **kwargs\n",
    "    ):\n",
    "        \n",
    "        self.node_num =node_num\n",
    "        self.seq_len = seq_len\n",
    "        self.n_first = n_first\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = n_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = activation_resolver.make(act)\n",
    "        self.act_first = act_first\n",
    "        self.eps = eps\n",
    "\n",
    "        if out_channels is not None:\n",
    "            self.out_channels = out_channels\n",
    "        else:\n",
    "            self.out_channels = hidden_channels\n",
    "            \n",
    "        assert n_layers >= 2 , \"intra and inter conv layers must greater than or equals to 2 \"\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(self.init_intra_conv(in_channels, hidden_channels))\n",
    "\n",
    "        for i in range(n_layers - 2):\n",
    "            if i % 2 == 1: # intra_conv\n",
    "                self.convs.append(self.init_intra_conv(in_channels, hidden_channels))\n",
    "            else: # inter conv\n",
    "                self.convs.append(self.init_inter_conv(hidden_channels, hidden_channels))\n",
    "          \n",
    "            \n",
    "        if n_layers % 2 == 1: # intra_conv\n",
    "            self.convs.append(self.init_intra_conv(in_channels, out_channels))\n",
    "        else: # inter conv\n",
    "            self.convs.append(self.init_inter_conv(hidden_channels, out_channels))\n",
    "\n",
    "        self.norms = None\n",
    "        if norm is not None:\n",
    "            self.norms = nn.ModuleList()\n",
    "            for _ in range(n_layers - 1):\n",
    "                self.norms.append(copy.deepcopy(norm))\n",
    "            \n",
    "    def init_intra_conv(self, in_channels, out_channels, **kwargs):\n",
    "        # print(\"init_intra\")\n",
    "        intrast_homo_conv = HeteroConv({\n",
    "            ('s', 's2s', 's'): SSConv(in_channels, out_channels, eps=self.eps),\n",
    "            ('t', 't2t', 't'): TTConv(in_channels, out_channels, eps=self.eps),\n",
    "        }, aggr='sum')\n",
    "        return intrast_homo_conv\n",
    "\n",
    "    def init_inter_conv(self, in_channels, out_channels, **kwargs):\n",
    "        # print(\"init_inter\")\n",
    "        interst_biparte_conv = HeteroConv({\n",
    "            ('s', 's2t', 't'): STConv(in_channels, out_channels, eps=self.eps),\n",
    "            ('t', 't2s', 's'): TSConv(in_channels, out_channels, eps=self.eps),\n",
    "        }, aggr='sum')\n",
    "        return interst_biparte_conv\n",
    "        # return FAConv(in_channels, out_channels, **kwargs)\n",
    "\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # x: B * (N+T) * C\n",
    "        # edge_index: B,2,2*(N*T)\n",
    "        # edge_attr: B*E or B * (N * T )\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            xs = list()\n",
    "            for bi in range(x.shape[0]):\n",
    "\n",
    "                x_dict = {\n",
    "                    's': x[bi][:self.node_num,:],\n",
    "                    't': x[bi][self.node_num:,:]\n",
    "                }\n",
    "                edge_index_bi = edge_index[bi]\n",
    "                if i % 2 == 0: # intra\n",
    "                    edge_nn = edge_index_bi[:, (edge_index_bi[0] < self.node_num) & (edge_index_bi[1] < self.node_num)]\n",
    "                    edge_tt = edge_index_bi[:, (edge_index_bi[0] >=self.node_num ) & (edge_index_bi[1]  >=self.node_num)]\n",
    "                    \n",
    "                    # set tt edge start index to 0\n",
    "                    edge_tt = edge_tt - self.node_num\n",
    "                    edge_index_dict = {\n",
    "                        ('s', 's2s', 's'): edge_nn,\n",
    "                        ('t', 't2t', 't'): edge_tt,\n",
    "                    }\n",
    "                    out_dict = self.convs[i](x_dict,edge_index_dict )\n",
    "                    xi = x[bi] + torch.concat([out_dict['s'], out_dict['t']], dim=0)\n",
    "                    # print(\"prop intra\")\n",
    "                    \n",
    "                else: # inter\n",
    "                    edge_nt = edge_index_bi[:, (edge_index_bi[0] < self.node_num) & (edge_index_bi[1] >= self.node_num)]\n",
    "                    edge_tn = edge_index_bi[:, (edge_index_bi[0] >= self.node_num) & (edge_index_bi[1] < self.node_num)]        \n",
    "                    edge_index_dict = {\n",
    "                        ('s', 's2t', 't'): edge_nt,\n",
    "                        ('t', 't2s', 's'): edge_tn,\n",
    "                    }\n",
    "                    # print(\"prop inter\")\n",
    "                    \n",
    "                    out_dict = self.convs[i](x_dict,edge_index_dict )\n",
    "                    \n",
    "                    xi = x[bi] + out_dict['s'] + out_dict['t']\n",
    "                # combining spatial and temporal mixed information\n",
    "                \n",
    "                # xi = self.convs[i](x[bi], edge_index[bi])\n",
    "                xs.append(xi)\n",
    "            x = torch.stack(xs)\n",
    "            if i == self.num_layers - 1:\n",
    "                break\n",
    "            \n",
    "            if self.act_first:\n",
    "                x = self.act(x)\n",
    "            if self.norms is not None:\n",
    "                x = self.norms[i](x)\n",
    "            if not self.act_first:\n",
    "                x = self.act(x)\n",
    "            \n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 2\n",
    "seq_len = 4\n",
    "batch_size = 8\n",
    "\n",
    "xn = torch.randn(n_nodes, 128)\n",
    "xt = torch.randn(seq_len, 128)\n",
    "xi = torch.concat([xn, xt], dim=0)\n",
    "\n",
    "\n",
    "tmp = torch.zeros((n_nodes + seq_len, n_nodes + seq_len)) # (NxT , NxT)\n",
    "tmp[:n_nodes, n_nodes:] = 1\n",
    "tmp[n_nodes:, :n_nodes] = 1\n",
    "tmp[:n_nodes, :n_nodes] = 1\n",
    "tmp[n_nodes:, n_nodes:] = 1\n",
    "tmp = tmp - torch.eye(n_nodes + seq_len,n_nodes + seq_len)\n",
    "edge_index = torch.nonzero(tmp).T\n",
    "# edge_index\n",
    "# [edge_index[0][edge_index[0] < n_nodes],\n",
    "# edge_index[1][edge_index[1] >= n_nodes]]\n",
    "\n",
    "\n",
    "\n",
    "batch_x = xi.expand(batch_size, -1, -1)\n",
    "batch_indices = edge_index.expand(batch_size, -1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_bi  = batch_indices[0]\n",
    "\n",
    "edge_nn = edge_index_bi[:, (edge_index_bi[0] < n_nodes) & (edge_index_bi[1] < n_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_intra\n",
      "init_inter\n",
      "init_intra\n"
     ]
    }
   ],
   "source": [
    "gcn = HeteroFASTGCN(n_nodes,seq_len, in_channels=128, hidden_channels=128, n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop inter\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n",
      "prop intra\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]],\n",
       "\n",
       "        [[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]],\n",
       "\n",
       "        [[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]],\n",
       "\n",
       "        [[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]],\n",
       "\n",
       "        [[ 0.0000e+00,  3.5309e-01,  0.0000e+00,  ...,  6.5688e-01,\n",
       "          -3.2479e-01,  3.9053e-01],\n",
       "         [ 0.0000e+00, -6.9949e-02,  0.0000e+00,  ..., -1.3013e-01,\n",
       "           7.6174e-01, -7.7367e-02],\n",
       "         [ 0.0000e+00,  7.5263e-02,  1.1883e-01,  ...,  1.4307e-01,\n",
       "           2.5159e-02,  1.3688e+00],\n",
       "         [ 0.0000e+00,  6.2398e-02,  3.7416e-01,  ..., -2.0323e-03,\n",
       "           7.1230e-02, -2.0018e-01],\n",
       "         [ 0.0000e+00, -4.1840e-04, -5.2703e-03,  ...,  7.3938e-01,\n",
       "          -4.8131e-03,  8.9747e-01],\n",
       "         [ 0.0000e+00,  2.2863e-02,  2.2337e-01,  ...,  7.8138e-03,\n",
       "           1.2037e-01,  3.1702e-03]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn(batch_x, batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "all_nt = 20\n",
    "n_nodes = 10\n",
    "t_nodes = 10\n",
    "n_embedding = torch.nn.Parameter(torch.rand(n_nodes, 16))\n",
    "\n",
    "data =torch.randn(64 , all_nt, 16)\n",
    "\n",
    "\n",
    "def build_adj(node_embs):\n",
    "    # 使用softmax处理输入\n",
    "    tmp = torch.relu(torch.einsum(\"bnf, bmf -> bnm\", data,data))\n",
    "    softmax_output = torch.softmax(tmp, dim=-1)\n",
    "    \n",
    "    # 将原始输入中为0的位置在输出中设置为0\n",
    "    softmax_output[tmp == 0] = 0\n",
    "\n",
    "    return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.7022e-01, 1.4890e-03, 1.9415e-03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 3.0381e-04],\n",
       "         [9.3984e-08, 9.9999e-01, 2.9526e-06,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0251e-06, 2.4698e-05, 9.9975e-01,  ..., 0.0000e+00,\n",
       "          1.5246e-04, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6907e-01,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 3.1669e-03,  ..., 0.0000e+00,\n",
       "          9.9311e-01, 0.0000e+00],\n",
       "         [4.2778e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 9.8225e-01]],\n",
       "\n",
       "        [[9.9403e-01, 4.0011e-04, 2.9565e-06,  ..., 0.0000e+00,\n",
       "          2.6809e-06, 1.2316e-04],\n",
       "         [4.3335e-03, 9.9048e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          2.8735e-05, 0.0000e+00],\n",
       "         [2.1323e-09, 0.0000e+00, 9.9983e-01,  ..., 3.1886e-09,\n",
       "          1.6799e-08, 8.8486e-06],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 9.6847e-10,  ..., 9.9994e-01,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.4774e-05, 7.3997e-05, 6.4967e-04,  ..., 0.0000e+00,\n",
       "          9.9016e-01, 1.4112e-04],\n",
       "         [4.0467e-12, 0.0000e+00, 4.0312e-10,  ..., 0.0000e+00,\n",
       "          1.6625e-13, 1.0000e+00]],\n",
       "\n",
       "        [[9.9890e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          7.3993e-08, 9.6540e-04],\n",
       "         [0.0000e+00, 9.9559e-01, 3.7693e-03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 2.9082e-04, 9.9934e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9858e-01,\n",
       "          1.8429e-05, 0.0000e+00],\n",
       "         [1.6252e-05, 0.0000e+00, 0.0000e+00,  ..., 1.0096e-04,\n",
       "          7.7452e-01, 0.0000e+00],\n",
       "         [2.8290e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 9.7404e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[9.7252e-01, 0.0000e+00, 0.0000e+00,  ..., 3.0005e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 9.9983e-01, 1.1124e-05,  ..., 0.0000e+00,\n",
       "          3.7924e-07, 0.0000e+00],\n",
       "         [0.0000e+00, 3.1849e-08, 9.9999e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.0437e-13, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          1.5772e-11, 8.1038e-13],\n",
       "         [0.0000e+00, 1.6098e-03, 0.0000e+00,  ..., 1.1776e-01,\n",
       "          8.2927e-01, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7996e-05,\n",
       "          0.0000e+00, 9.9983e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 6.9082e-15,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 9.9986e-01, 1.8421e-07,  ..., 6.3122e-08,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [6.8627e-06, 1.3304e-06, 9.9979e-01,  ..., 0.0000e+00,\n",
       "          7.6230e-06, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 2.5491e-09, 0.0000e+00,  ..., 9.9999e-01,\n",
       "          3.5889e-09, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 2.0909e-06,  ..., 1.7605e-07,\n",
       "          9.9659e-01, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 9.9418e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          5.0897e-12, 0.0000e+00],\n",
       "         [0.0000e+00, 1.0000e+00, 1.1694e-07,  ..., 0.0000e+00,\n",
       "          2.1836e-06, 0.0000e+00],\n",
       "         [0.0000e+00, 1.8222e-03, 9.9168e-01,  ..., 0.0000e+00,\n",
       "          5.1722e-04, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9833e-01,\n",
       "          0.0000e+00, 8.3970e-04],\n",
       "         [9.2013e-07, 8.1367e-03, 1.2368e-04,  ..., 0.0000e+00,\n",
       "          9.9104e-01, 8.7807e-06],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0583e-07,\n",
       "          1.5932e-09, 1.0000e+00]]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = build_adj(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.8180,  1.4516,  0.0000,  0.0000,  8.7246,  3.7397,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  7.9727,  0.0000,  0.0000,  4.1917,  7.5985,  4.5027,\n",
       "          0.0000,  0.0000,  6.6029,  0.0000],\n",
       "        [ 1.4516, 14.9157,  0.0000,  2.6948,  1.9211,  5.7597,  0.0000,  0.0000,\n",
       "          3.7293,  0.0000,  5.8095,  0.0000,  0.0000,  0.0000,  2.6841,  0.7558,\n",
       "          0.0000,  0.0000,  0.0000,  1.5335],\n",
       "        [ 0.0000,  0.0000, 19.8097,  0.0000,  1.1402,  0.0000,  4.5790,  1.7512,\n",
       "          9.9575,  5.9772,  0.0000,  5.6550,  3.3784,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  2.6948,  0.0000, 12.7121,  0.8827,  0.0000,  0.0000,  0.9208,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  5.6144,  0.2867,  0.0000,  0.0000,\n",
       "          0.2528,  0.0000,  0.0000,  8.2223],\n",
       "        [ 8.7246,  1.9211,  1.1402,  0.8827, 15.8133,  2.0579,  0.8128,  0.0000,\n",
       "          1.7998,  0.0000,  4.5066,  0.0000,  3.7069,  0.0000,  2.4055,  0.2129,\n",
       "          0.0000,  0.0000,  0.4136,  0.0000],\n",
       "        [ 3.7397,  5.7597,  0.0000,  0.0000,  2.0579,  9.5879,  0.0000,  0.0000,\n",
       "          0.2851,  0.0000,  4.8557,  0.0000,  0.0000,  0.0000,  3.0503,  0.5260,\n",
       "          0.0000,  4.0990,  0.4087,  0.0000],\n",
       "        [ 0.0000,  0.0000,  4.5790,  0.0000,  0.8128,  0.0000, 20.9405,  4.6845,\n",
       "          0.0000,  0.0000,  0.0000,  6.4371,  1.1563,  0.0000,  2.3302,  0.0000,\n",
       "          1.2075,  0.0000,  0.0000,  0.7118],\n",
       "        [ 0.0000,  0.0000,  1.7512,  0.9208,  0.0000,  0.0000,  4.6845, 10.0977,\n",
       "          0.0000,  0.0000,  0.0000,  9.2799,  3.4940,  2.7213,  0.0000,  2.8860,\n",
       "          0.0000,  0.0000,  0.0000,  1.0641],\n",
       "        [ 0.0000,  3.7293,  9.9575,  0.0000,  1.7998,  0.2851,  0.0000,  0.0000,\n",
       "         19.5628,  3.2301,  0.0000,  0.0000,  9.4144,  0.0000,  5.3622,  0.1924,\n",
       "          2.1203,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  5.9772,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          3.2301, 14.7552,  0.0000,  1.2165,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  3.1492,  0.0000],\n",
       "        [ 7.9727,  5.8095,  0.0000,  0.0000,  4.5066,  4.8557,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, 13.6115,  0.0000,  0.0000,  4.4718,  0.0000,  3.2249,\n",
       "          0.0000,  7.7821,  3.1577,  0.0000],\n",
       "        [ 0.0000,  0.0000,  5.6550,  0.0000,  0.0000,  0.0000,  6.4371,  9.2799,\n",
       "          0.0000,  1.2165,  0.0000, 29.5312,  1.8390,  4.2714,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.6226],\n",
       "        [ 0.0000,  0.0000,  3.3784,  5.6144,  3.7069,  0.0000,  1.1563,  3.4940,\n",
       "          9.4144,  0.0000,  0.0000,  1.8390, 26.9759,  0.0000,  6.9050,  2.8526,\n",
       "          6.1247,  0.0000,  0.0000,  2.9393],\n",
       "        [ 4.1917,  0.0000,  0.0000,  0.2867,  0.0000,  0.0000,  0.0000,  2.7213,\n",
       "          0.0000,  0.0000,  4.4718,  4.2714,  0.0000, 20.9850,  0.0000,  3.2241,\n",
       "          0.0000,  0.0000,  6.7837,  0.0000],\n",
       "        [ 7.5985,  2.6841,  0.0000,  0.0000,  2.4055,  3.0503,  2.3302,  0.0000,\n",
       "          5.3622,  0.0000,  0.0000,  0.0000,  6.9050,  0.0000, 34.0258,  1.7672,\n",
       "          7.1977,  0.0000,  0.0000,  0.8718],\n",
       "        [ 4.5027,  0.7558,  0.0000,  0.0000,  0.2129,  0.5260,  0.0000,  2.8860,\n",
       "          0.1924,  0.0000,  3.2249,  0.0000,  2.8526,  3.2241,  1.7672, 11.3429,\n",
       "          2.8294,  0.0000,  0.4966,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.2528,  0.0000,  0.0000,  1.2075,  0.0000,\n",
       "          2.1203,  0.0000,  0.0000,  0.0000,  6.1247,  0.0000,  7.1977,  2.8294,\n",
       "         17.4399,  2.0112,  0.0000,  7.1162],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0990,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  7.7821,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          2.0112, 17.9750,  0.0652,  2.9344],\n",
       "        [ 6.6029,  0.0000,  0.0000,  0.0000,  0.4136,  0.4087,  0.0000,  0.0000,\n",
       "          0.0000,  3.1492,  3.1577,  0.0000,  0.0000,  6.7837,  0.0000,  0.4966,\n",
       "          0.0000,  0.0652, 13.1569,  0.0000],\n",
       "        [ 0.0000,  1.5335,  0.0000,  8.2223,  0.0000,  0.0000,  0.7118,  1.0641,\n",
       "          0.0000,  0.0000,  0.0000,  0.6226,  2.9393,  0.0000,  0.8718,  0.0000,\n",
       "          7.1162,  2.9344,  0.0000, 14.6003]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "biadj = adj.detach().clone()\n",
    "biadj[:, :n_nodes, :n_nodes] = 0\n",
    "biadj[:, n_nodes:, n_nodes:] = 0\n",
    "\n",
    "\n",
    "topk_values, topk_indices = torch.topk(biadj, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 5])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "def topk_elements(matrix, k):\n",
    "    values, indices = torch.topk(matrix.view(-1), k)\n",
    "\n",
    "    result = torch.zeros_like(matrix)\n",
    "\n",
    "    flat_result = result.view(-1)\n",
    "    flat_result[indices] = values\n",
    "\n",
    "    return result\n",
    "rate = 0.5\n",
    "k = int(rate * 2 * n_nodes * t_nodes)\n",
    "for bi in range(batch_size):\n",
    "    biadj[bi] = topk_elements(biadj[bi], k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9221e-03, 2.5567e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.9786e-04, 4.3862e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         6.7577e-03, 0.0000e+00, 2.0685e-03, 0.0000e+00, 0.0000e+00, 2.1023e-04,\n",
       "         9.7227e-04, 2.7818e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.3110e-04, 1.0145e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 8.3319e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9336e-05, 1.1405e-02,\n",
       "         1.7993e-04, 1.4373e-03, 2.9243e-05, 0.0000e+00, 1.1186e-02, 0.0000e+00,\n",
       "         0.0000e+00, 3.8914e-05],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.2800e-04, 3.3961e-03, 2.5324e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.2913e-05, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3944e-04, 2.9930e-01,\n",
       "         0.0000e+00, 2.7558e-04, 8.0537e-04, 3.8509e-02, 1.6969e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.0603e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4936e-02,\n",
       "         1.0814e-04, 7.4208e-04, 1.0705e-03, 1.4210e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.1749e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.0383e-03, 3.6191e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4079e-03,\n",
       "         0.0000e+00, 3.2327e-05, 0.0000e+00, 6.1760e-04, 2.2335e-03, 3.2962e-05,\n",
       "         0.0000e+00, 6.2493e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.9356e-05, 0.0000e+00, 1.8965e-04, 4.2156e-04, 1.6415e-04, 1.3566e-04,\n",
       "         0.0000e+00, 6.5421e-03],\n",
       "        [1.9221e-03, 0.0000e+00, 0.0000e+00, 6.9336e-05, 0.0000e+00, 8.3944e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.5567e-03, 0.0000e+00, 0.0000e+00, 1.1405e-02, 0.0000e+00, 2.9930e-01,\n",
       "         9.4936e-02, 0.0000e+00, 9.4079e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 6.7577e-03, 3.3110e-04, 1.7993e-04, 2.2800e-04, 0.0000e+00,\n",
       "         1.0814e-04, 0.0000e+00, 0.0000e+00, 5.9356e-05, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0145e-04, 1.4373e-03, 3.3961e-03, 2.7558e-04,\n",
       "         7.4208e-04, 1.0383e-03, 3.2327e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 2.0685e-03, 0.0000e+00, 2.9243e-05, 2.5324e-03, 8.0537e-04,\n",
       "         1.0705e-03, 3.6191e-03, 0.0000e+00, 1.8965e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8509e-02,\n",
       "         1.4210e-03, 0.0000e+00, 6.1760e-04, 4.2156e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1186e-02, 0.0000e+00, 1.6969e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.2335e-03, 1.6415e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 2.1023e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.2962e-05, 1.3566e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.9786e-04, 9.7227e-04, 0.0000e+00, 0.0000e+00, 7.2913e-05, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.3862e-04, 2.7818e-04, 8.3319e-04, 3.8914e-05, 0.0000e+00, 1.0603e-04,\n",
       "         5.1749e-04, 0.0000e+00, 6.2493e-04, 6.5421e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biadj[bi] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "biadj[ind] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj[ind] = 0\n",
    "\n",
    "mask =  torch.zeros_like(adj)\n",
    "\n",
    "mask = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biparte build : select top k \n",
    "biadj = adj.detach().clone()\n",
    "biadj[:, :n_nodes, :n_nodes] = 0\n",
    "biadj[:, n_nodes:, n_nodes:] = 0\n",
    "def topk_elements(matrix, k):\n",
    "    values, indices = torch.topk(matrix.view(-1), k)\n",
    "\n",
    "    result = torch.zeros_like(matrix)\n",
    "\n",
    "    flat_result = result.view(-1)\n",
    "    flat_result[indices] = values\n",
    "\n",
    "    return result\n",
    "\n",
    "k = int(edge_rate * 2 * n_nodes * seq_len)\n",
    "for bi in range(batch_size):\n",
    "    biadj[bi] = topk_elements(biadj[bi], k)\n",
    "adj[:, :n_nodes, :n_nodes] = biadj[:, :n_nodes, :n_nodes]\n",
    "adj[:, n_nodes:, n_nodes:] = biadj[:, n_nodes:, n_nodes:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/notebooks/pytorch_timeseries/notebooks/04_st_conv_st.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B251-internal-yww-jupyter/notebooks/pytorch_timeseries/notebooks/04_st_conv_st.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m source_nodes, target_nodes \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39mnonzero()\u001b[39m.\u001b[39mt()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# source_nodes, target_nodes = adj.nonzero().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
